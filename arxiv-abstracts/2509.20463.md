# Efficiently Attacking Memorization Scores

## Title

Efficiently Attacking Memorization Scores

## Metadata

- **paper_id**: `2509.20463`
- **arXiv_abs**: https://arxiv.org/abs/2509.20463
- **arXiv_pdf**: https://arxiv.org/pdf/2509.20463.pdf
- **authors**: Tue Do, Varun Chandrasekaran, Daniel Alabi
- **categories**: cs.LG
- **created**: `2025-09-24T18:33:10+00:00`
- **updated**: `2025-09-29T05:41:29+00:00`
- **datestamp**: `2025-09-30`
- **setspecs**: cs:cs:LG
- **exported_at**: `2026-01-24T01:01:20.754878+00:00`

## Abstract

Influence estimation tools -- such as memorization scores -- are widely used to understand model behavior, attribute training data, and inform dataset curation. However, recent applications in data valuation and responsible machine learning raise the question: can these scores themselves be adversarially manipulated? In this work, we present a systematic study of the feasibility of attacking memorization-based influence estimators. We characterize attacks for producing highly memorized samples as highly sensitive queries in the regime where a trained algorithm is accurate. Our attack (calculating the pseudoinverse of the input) is practical, requiring only black-box access to model outputs and incur modest computational overhead. We empirically validate our attack across a wide suite of image classification tasks, showing that even state-of-the-art proxies are vulnerable to targeted score manipulations. In addition, we provide a theoretical analysis of the stability of memorization scores under adversarial perturbations, revealing conditions under which influence estimates are inherently fragile. Our findings highlight critical vulnerabilities in influence-based attribution and suggest the need for robust defenses. All code can be found at https://github.com/tuedo2/MemAttack
