# clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations

## Title

clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations

## Metadata

- **paper_id**: `2505.05445`
- **arXiv_abs**: https://arxiv.org/abs/2505.05445
- **arXiv_pdf**: https://arxiv.org/pdf/2505.05445.pdf
- **authors**: Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen
- **categories**: cs.CL
- **created**: `2025-05-08T17:36:36+00:00`
- **updated**: `2025-07-21T12:42:09+00:00`
- **datestamp**: `2025-07-22`
- **setspecs**: cs:cs:CL
- **exported_at**: `2026-01-24T01:01:20.738539+00:00`

## Abstract

The emergence of instruction-tuned large language models (LLMs) has advanced the field of dialogue systems, enabling both realistic user simulations and robust multi-turn conversational agents. However, existing research often evaluates these components in isolation-either focusing on a single user simulator or a specific system design-limiting the generalisability of insights across architectures and configurations. In this work, we propose clem todd (chat-optimized LLMs for task-oriented dialogue systems development), a flexible framework for systematically evaluating dialogue systems under consistent conditions. clem todd enables detailed benchmarking across combinations of user simulators and dialogue systems, whether existing models from literature or newly developed ones. It supports plug-and-play integration and ensures uniform datasets, evaluation metrics, and computational constraints. We showcase clem todd's flexibility by re-evaluating existing task-oriented dialogue systems within this unified setup and integrating three newly proposed dialogue systems into the same evaluation pipeline. Our results provide actionable insights into how architecture, scale, and prompting strategies affect dialogue performance, offering practical guidance for building efficient and effective conversational AI systems.
