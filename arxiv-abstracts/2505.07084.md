# DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models

## Title

DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models

## Metadata

- **paper_id**: `2505.07084`
- **arXiv_abs**: https://arxiv.org/abs/2505.07084
- **arXiv_pdf**: https://arxiv.org/pdf/2505.07084.pdf
- **authors**: Shucheng Huang, Freda Shi, Chen Sun, Jiaming Zhong, Minghao Ning, Yufeng Yang, Yukun Lu, Hong Wang, Amir Khajepour
- **categories**: cs.RO
- **created**: `2025-05-11T18:14:33+00:00`
- **updated**: `2025-09-09T06:37:28+00:00`
- **datestamp**: `2025-09-12`
- **doi**: `10.1109/TVT.2025.3608811`
- **setspecs**: cs:cs:RO
- **exported_at**: `2026-01-24T01:01:20.738539+00:00`

## Abstract

Human drivers possess spatial and causal intelligence, enabling them to perceive driving scenarios, anticipate hazards, and react to dynamic environments. In contrast, autonomous vehicles lack these abilities, making it challenging to manage perception-related Safety of the Intended Functionality (SOTIF) risks, especially under complex or unpredictable driving conditions. To address this gap, we propose fine-tuning multimodal large language models (MLLMs) on a customized dataset specifically designed to capture perception-related SOTIF scenarios. Benchmarking results show that fine-tuned MLLMs achieve an 11.8\% improvement in close-ended VQA accuracy and a 12.0\% increase in open-ended VQA scores compared to baseline models, while maintaining real-time performance with a 0.59-second average inference time per image. We validate our approach through real-world case studies in Canada and China, where fine-tuned models correctly identify safety risks that challenge even experienced human drivers. This work represents the first application of domain-specific MLLM fine-tuning for SOTIF domain in autonomous driving. The dataset and related resources are available at github.com/s95huang/DriveSOTIF.git
