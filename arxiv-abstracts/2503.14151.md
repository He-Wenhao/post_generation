# Concat-ID: Towards Universal Identity-Preserving Video Synthesis

## Title

Concat-ID: Towards Universal Identity-Preserving Video Synthesis

## Metadata

- **paper_id**: `2503.14151`
- **arXiv_abs**: https://arxiv.org/abs/2503.14151
- **arXiv_pdf**: https://arxiv.org/pdf/2503.14151.pdf
- **authors**: Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, Chongxuan Li
- **categories**: cs.CV, cs.AI
- **created**: `2025-03-18T11:17:32+00:00`
- **updated**: `2025-07-02T11:55:35+00:00`
- **datestamp**: `2025-07-03`
- **setspecs**: cs:cs:CV, cs:cs:AI
- **exported_at**: `2026-01-24T01:01:20.738539+00:00`

## Abstract

We present Concat-ID, a unified framework for identity-preserving video generation. Concat-ID employs variational autoencoders to extract image features, which are then concatenated with video latents along the sequence dimension. It relies exclusively on inherent 3D self-attention mechanisms to incorporate them, eliminating the need for additional parameters or modules. A novel cross-video pairing strategy and a multi-stage training regimen are introduced to balance identity consistency and facial editability while enhancing video naturalness. Extensive experiments demonstrate Concat-ID's superiority over existing methods in both single and multi-identity generation, as well as its seamless scalability to multi-subject scenarios, including virtual try-on and background-controllable generation. Concat-ID establishes a new benchmark for identity-preserving video synthesis, providing a versatile and scalable solution for a wide range of applications.
