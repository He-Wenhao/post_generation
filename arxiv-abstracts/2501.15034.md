# Divergence-Augmented Policy Optimization

## Title

Divergence-Augmented Policy Optimization

## Metadata

- **paper_id**: `2501.15034`
- **arXiv_abs**: https://arxiv.org/abs/2501.15034
- **arXiv_pdf**: https://arxiv.org/pdf/2501.15034.pdf
- **authors**: Qing Wang, Yingru Li, Jiechao Xiong, Tong Zhang
- **categories**: cs.LG, cs.AI, stat.ML
- **created**: `2025-01-25T02:35:46+00:00`
- **updated**: `2025-01-25T02:35:46+00:00`
- **datestamp**: `2025-01-28`
- **setspecs**: cs:cs:LG, cs:cs:AI, stat:stat:ML
- **exported_at**: `2026-01-24T01:01:20.734101+00:00`

## Abstract

In deep reinforcement learning, policy optimization methods need to deal with issues such as function approximation and the reuse of off-policy data. Standard policy gradient methods do not handle off-policy data well, leading to premature convergence and instability. This paper introduces a method to stabilize policy optimization when off-policy data are reused. The idea is to include a Bregman divergence between the behavior policy that generates the data and the current policy to ensure small and safe policy updates with off-policy data. The Bregman divergence is calculated between the state distributions of two policies, instead of only on the action probabilities, leading to a divergence augmentation formulation. Empirical experiments on Atari games show that in the data-scarce scenario where the reuse of off-policy data becomes necessary, our method can achieve better performance than other state-of-the-art deep reinforcement learning algorithms.
